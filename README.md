<h1 align="center">Transformers Ambarella Extension</h1>

# üìùIntroduction
The Transformers Ambarella Extension (`transformers_amba_ext`) package provides application programming interfaces (APIs) to run open models, generated by the Ambarella VLMGen tool for performing text and vision tasks on different modalities. These models can be applied to:
* Text: Tasks such as text classification, information extraction, question answering, summarization, translation, and text generation

	[Llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b)

	[Llama-2-13B](https://huggingface.co/meta-llama/Llama-2-13b)

	[CodeLlama-13b-hf](https://huggingface.co/codellama/CodeLlama-13b-hf)

	[Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)

	[gemma-7b-it](https://huggingface.co/google/gemma-7b-it)

	[gemma-2-2b-it](https://huggingface.co/google/gemma-2-2b-it)

	[Phi-3-mini-128k-instruct](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct)

	[Qwen1.5-7B-Chat](https://huggingface.co/Qwen/Qwen1.5-7B-Chat)

	[Qwen2-7B-Instruct](https://huggingface.co/Qwen/Qwen2-7B-Instruct)

	[DeepSeek-R1-Distill-Qwen-7B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)

	[DeepSeek-R1-Distill-Qwen-32B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)

	[TinyLlama-1.1B-Chat-v1.0](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)

	[gemma-2b-it](https://huggingface.co/google/gemma-2b-it)

	[Phi-3-mini-4k-instruct](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)

	[Qwen2-0.5B-Instruct](https://huggingface.co/Qwen/Qwen2-0.5B-Instruct)

	[Qwen2-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2-1.5B-Instruct)

	[DeepSeek-R1-Distill-Qwen-1.5B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)

* Vision: Tasks such as image classification, object detection, and segmentation

	[llava-v1.5-13b](https://huggingface.co/liuhaotian/llava-v1.5-13b)

	[llava-onevision-qwen2-7b-ov-chat](https://huggingface.co/lmms-lab/llava-onevision-qwen2-7b-ov-chat)


# üì¶Installation
Use `pip` to install the wheel package.
```
# Install the dependencies
(ubuntu)# pip3 install -r requirements.txt

# Build and install the wheel package
(ubuntu)# make build
(ubuntu)# pip3 install dist/transformers_amba_ext-<version>.<date>-py3-none-any.whl
```

# üìÉDocumentation
Use Sphinx to generate a document from the source code package.
```
(ubuntu)# pip3 install sphinx recommonmark sphinx-markdown-tables sphinx_book_theme
(ubuntu)# make doc

## Then open `docs/build/html/index.html` in a web browser to view the document.
```

# üî•Quick start
The `transformers_amba_ext` package provides APIs to quickly use pre-trained models on texts or images; it inherits the same "Transformers" model name and rewrites commonly used interfaces such as `from_pretrained` and `generate`. The extended configurations, `device_ip`, `device_port`, `log_level`, and `vit_mode`, are available for the rewritten APIs. In the source package, users can view the introduction and sample code that includes the different model classes such as `LlamaForCausalLM`, `LlavaLlamaForCausalLM`, and `LlavaOnevisionForConditionalGeneration`.
```
From transformers import AutoTokenizer
# Change to use the LlamaForCausalLM class from the transformers_amba_ext package
# From transformers import LlamaForCausalLM
From transformers_amba_ext import LlamaForCausalLM

model_path = "/home/lychee/cooper_max_demos/llm_demo/chatllama_7B/"
model = LlamaForCausalLM.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)

prompt = "[INST] What is the height of Michael Jeffrey Jordan? [/INST]"
inputs = tokenizer(prompt, return_tensors="pt")

generate_ids = model.generate(inputs.input_ids)
response = tokenizer.batch_decode(
generate_ids[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
print(response)
```

# üíñBenchmark
The verified benckmarks are listed below; users can refer to the benckmark folder for more information.
* [bigcode-evaluation-harness](https://github.com/bigcode-project/bigcode-evaluation-harness.git): Code Generation LM Evaluation Harness
* [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness.git): Language Model Evaluation Harness
* [lmms-eval](https://github.com/EvolvingLMMs-Lab/lmms-eval): The Evaluation Suite of Large Multimodal Models

# üêûQuality Assurance (QA)

1. To resolve the error below, users must create a soft link from `model_desc.json` or `info/config.json` to `config.json` in the <model_path>:
`ValueError: Unrecognized model in <model_path>. Should have a "model_type" key in its config.json`

2. To resolve the error below, users must create a soft link from `info/tokenizer.json` to `tokenizer.json` in the <model_path>:
`OSError: Can't load tokenizer for '<model_path>'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '<model_path>' is the correct path to a directory containing all relevant files for a <model_architectures> tokenizer.`

3. How to add a new text model from Shepherd?

   Users must follow the same Transformers folder structure and naming rules if they add a new text model to this package. First, users must get the architecture name for the current model from `model_desc.json` to check if it is supported or not. Otherwise, users can try to add it as follows:
   * Add a new folder in the `transformers_amba_ext/models` folder and the name should come from Transformers.
   * Add `__init__.py` and `modeling_xxx.py` in the new folder and create a new class (`architecture name`) that can inherit from the `LlamaForCausalLM` class, then export the new class in the `__init__.py` file. Users can refer to `transformers_amba_ext/models/qwen2` for more information.
   * Export the current new model in both `transformers_amba_ext/__init__.py` and `transformers_amba_ext/models/__init__.py`.

4. How to run an large language model (LLM) on a remote PC? <a id="Section_Enable_RPC"></a>

   The Shepherd library supports remote procedure call (RPC) mode, enabling users to run an LLM model on a remote PC. Usage information is available in the Ambarella software developer kit (SDK) documentation. Users can follow the steps below to enable this feature:
   1. Build Shepherd on a PC and export `libshepherd.so` to the system environment by using `source export_lib.env`.
     ```
     (ubuntu)# `cd ${COOPER_SDK}/ambarella/packages/shepherd/rpc`
     (ubuntu)# `make`
     (ubuntu)# `source export_lib.env`
     ```
   2. Set up the `shepherd_device_daemon` application on the Ambarella board.
     ```
     (board)# `shepherd_device_daemon -m /tmp --keep-model --port 8890`
     ```
   3. Set `device_ip=${BOARD_IP}` and `device_port=8890` for the `model.from_pretrained` API, then users can run an LLM model on a remote PC. Users can get an example case with `unit_test/test_chat.py`.

