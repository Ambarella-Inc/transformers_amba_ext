<h1 align="center">Transformers Ambarella Extension</h1>

# üìùIntroduction
Transformers Ambarella Extension (transformers_amba_ext) package provides below pretrained models which generated by the HayPlus tool from Ambarella to perform tasks on different modalities such as text, vision. These models can be applied on:
* text, for tasks like text classification, information extraction, question answering, summarization, translation, and text generation.

	[Llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b)

	[Llama-2-13B](https://huggingface.co/meta-llama/Llama-2-13b)

	[CodeLlama-13b-hf](https://huggingface.co/codellama/CodeLlama-13b-hf)

	[Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)

	[gemma-7b-it](https://huggingface.co/google/gemma-7b-it)

	[gemma-2-2b-it](https://huggingface.co/google/gemma-2-2b-it)

	[Phi-3-mini-128k-instruct](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct)

	[Qwen1.5-7B-Chat](https://huggingface.co/Qwen/Qwen1.5-7B-Chat)

	[Qwen2-7B-Instruct](https://huggingface.co/Qwen/Qwen2-7B-Instruct)

	[DeepSeek-R1-Distill-Qwen-7B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)

	[DeepSeek-R1-Distill-Qwen-32B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)

	[TinyLlama-1.1B-Chat-v1.0](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)

	[gemma-2b-it](https://huggingface.co/google/gemma-2b-it)

	[Phi-3-mini-4k-instruct](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)

	[Qwen2-0.5B-Instruct](https://huggingface.co/Qwen/Qwen2-0.5B-Instruct)

	[Qwen2-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2-1.5B-Instruct)

	[DeepSeek-R1-Distill-Qwen-1.5B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)

* vision, for tasks like image classification, object detection, and segmentation.

	[llava-v1.5-13b](https://huggingface.co/liuhaotian/llava-v1.5-13b)

	[llava-onevision-qwen2-7b-ov-chat](https://huggingface.co/lmms-lab/llava-onevision-qwen2-7b-ov-chat)


# üì¶Installation
Use the pip tool to install the wheel package.
```
# install the dependencies
(ubuntu)# pip3 install -r requirements.txt

# build and install wheel package
(ubuntu)# make build
(ubuntu)# pip3 install dist/transformers_amba_ext-<version>.<date>-py3-none-any.whl
```

# üìÉDocument
Use the sphinx tool to generate the document from source code package.
```
(ubuntu)# pip3 install sphinx recommonmark sphinx-markdown-tables sphinx_book_theme
(ubuntu)# make doc

## Then, the user can open `docs/build/html/index.html` in a browser.
```

# üî•Quick start
The transformers_amba_ext package provides APIs to quickly use those pretrained models on a given text or image. It inherits the same model name as Transformers and rewrote commonly used interfaces such as `from_pretrained` and `generate`. There are also some extended configurations like `device_ip`, `device_port`, `log_level`, `vit_mode` for those rewrote API. Users could get the introduction or the sample code as below from the source package with different model classes such as `LlamaForCausalLM`, `LlavaLlamaForCausalLM` and `LlavaOnevisionForConditionalGeneration`.
```
from transformers import AutoTokenizer
# change to use LlamaForCausalLM class from transformers_amba_ext package
# from transformers import LlamaForCausalLM
from transformers_amba_ext import LlamaForCausalLM

model_path = "/home/lychee/cooper_max_demos/llm_demo/chatllama_7B/"
model = LlamaForCausalLM.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)

prompt = "[INST] What is the height of Michael Jeffrey Jordan? [/INST]"
inputs = tokenizer(prompt, return_tensors="pt")

generate_ids = model.generate(inputs.input_ids)
response = tokenizer.batch_decode(
generate_ids[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
print(response)
```

# üíñBenchmark
The verified benckmark are listed as the below, please reference as benckmark folder to get more information.
1. [bigcode-evaluation-harness](https://github.com/bigcode-project/bigcode-evaluation-harness.git): Code Generation LM Evaluation Harness
2. [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness.git): Language Model Evaluation Harness
3. [lmms-eval](https://github.com/EvolvingLMMs-Lab/lmms-eval): The Evaluation Suite of Large Multimodal Models

# üêûQA

1. Users need to create a soft link from model_desc.json or info/config.json to config.json in the <model_path> for the following issue:
`ValueError: Unrecognized model in <model_path>. Should have a "model_type" key in its config.json`

2. Users need to create a soft link from info/tokenizer.json to tokenizer.json in the <model_path> for the following issue:
`OSError: Can't load tokenizer for '<model_path>'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '<model_path>' is the correct path to a directory containing all relevant files for a <model_architectures> tokenizer.`

3. How to add a new text model from Shepherd?

   Users should follow the same folder struct and name rule with Transformers if they need to add a new text model to this package. Firstly, users need to get the architecture name for the current model from `model_desc.json` to check if it is supported or not. If not, then we can try to add it as below:
   * add a new folder in the `transformers_amba_ext/models` folder and the name could come from Transformers.
   * add `__init__.py` and `modeling_xxx.py` in the new folder and create a new class(`architectures name`) which can inherit `LlamaForCausalLM` class, then export the new class in the `__init__.py` file. Users can refer to `transformers_amba_ext/models/qwen2` to get more information.
   * export the current new model in the `transformers_amba_ext/__init__.py` and `transformers_amba_ext/models/__init__.py`

4. How to run an LLM model on remote PC? <a id="Section_Enable_RPC"></a>

   Shepherd library has already supported RPC mode and it means users could run an LLM model on a remote PC, and the usage can be obtained from the document of Ambarella SDK. Users can follow below steps if they want to enable this feature:
   * build Shepherd on PC and export libshepherd.so to system environment with using `source export_lib.env`.
     ```
     (ubuntu)# cd ${COOPER_SDK}/ambarella/packages/shepherd/rpc
     (ubuntu)# make
     (ubuntu)# source export_lib.env
     ```
   * setup shepherd_device_daemon app on Ambarella board.
     ```
     (board)# shepherd_device_daemon -m /tmp --keep-model --port 8890
     ```
   * set `device_ip=${BOARD_IP}` and `device_port=8890` for `model.from_pretrained` API, then users can run an LLM model on remote PC. Users can get a example case with `unit_test/test_chat.py`.

