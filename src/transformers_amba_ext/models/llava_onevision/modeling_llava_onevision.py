
import numpy as np
import torch
from typing import Optional, Dict, Any
from transformers import TextIteratorStreamer

from ...utils import logging
from ..model_base.modeling_base import model_base
from ...inference.infer_configuration import ov_vit_mode
from ...inference.infer_configuration import LLAVA_OV_MODEL_TYPE_NAME

logger = logging.get_logger(__name__)

def check_image_type(image_tensor, vit_mode):
	if type(image_tensor) != torch.Tensor and type(image_tensor) != np.ndarray:
		logger.error("llava-onevision: the type of input images should be torch.Tensor or np.ndarray")
		return None

	if image_tensor.ndim != 4:
		logger.error("llava-onevision: the dimension of input images should be 4")
		return None

	if isinstance(image_tensor, torch.Tensor):
		image_tensor = image_tensor.numpy().astype(np.uint8)

	IMG_N, IMG_C, IMG_H, IMG_W = \
		image_tensor.shape[0], image_tensor.shape[1], image_tensor.shape[2], image_tensor.shape[3]

	if vit_mode == ov_vit_mode.VIT_SINGLE_IMG_MODE:
		if IMG_N != 1 or IMG_C != 3 or IMG_H != 720 or IMG_W != 1280:
			logger.error(
			f"llava-onevision: vit_mode: {vit_mode}, "
			f"single image mode requires it's resolution 1x3x720x1280, "
			f"but current input is: {IMG_N},{IMG_C},{IMG_H},{IMG_W}")
			return None
	elif vit_mode == ov_vit_mode.VIT_MULTI_IMG_MODE or vit_mode == ov_vit_mode.VIT_VIDEO_MODE:
		if IMG_C != 3 or IMG_H != 384 or IMG_W != 384:
			logger.error(
				f"llava-onevision: vit_mode: {vit_mode}, "
				f"multi image mode or video mode requires it's resolution Nx3x384x384, "
				f"but current input is: {IMG_N},{IMG_C},{IMG_H},{IMG_W}")
			return None
	else:
		logger.error("llava-onevision: unsupported vit mode: {vit_mode}")
		return None

	return image_tensor

class LlavaOnevisionForConditionalGeneration(model_base):
	def __init__(
		self,
		pretrained_model_path: str,
		device_ip: str = None,
		device_port: int = None,
		log_level: Optional[int] = None,
	):
		self.model_type = LLAVA_OV_MODEL_TYPE_NAME
		super().__init__(
			pretrained_model_path = pretrained_model_path,
			device_ip = device_ip,
			device_port = device_port,
			log_level = log_level,
			model_type = self.model_type)

	@classmethod
	def from_pretrained(
		cls,
		pretrained_model_path: Optional[str] = None,
		device_ip: Optional[str] = None,
		device_port: Optional[int] = None,
		log_level: Optional[int] = None,
		**kwargs,
	):
		r"""
		Args:
			pretrained_model_path (`str`, *optional*):
				Indices the model path which generated by Ambarella tool.
			device_ip (`str`, *optional*):
				It's an extended configuration for Ambarella chips to index the IP address if enable RPC mode.
			device_port (`int`, *optional*):
				It's an extended configuration for Ambarella chips to index the port number if enable RPC mode.
			log_level (`int`, *optional*):
				It's an extended configuration for Ambarella chips to index the log level for Shepherd library.
		"""
		if kwargs:
			logger.warning(f"{cls.__name__}: unsupported kwargs: {kwargs}")
		return cls(pretrained_model_path, device_ip, device_port, log_level)

	def __del__(self):
		super().__del__()

	def tokenizer_image_token(
		self,
		img_tensor: torch.Tensor = None,
		vit_mode: Optional[int] = None,
		user_id: Optional[list] = None
	):
		r"""This API will run vision tower model to get image embedding for Shepherd library.
		Users should use model.encode API to get text tokens instead of using tokenizer from Transformers
		because some related state for image was maintained by Shepherd.

		Args:
			img_tensor (`torch.Tensor`):
				Indices the input image data.
			vit_mode (`int`, *optional*):
				It's an extended configuration for Ambarella chips to index the vit mode for vision tower model.
				0: single image; 1: multi image; 2: video. Default is multi image mode.
			user_id (`list`, *optional*):
				It's an extended configuration for Ambarella chips to index the user ID for current inference.
				Users need specify this parameters if enable multi user.
		"""
		if img_tensor is None:
			logger.error("tokenizer_image_token: input with img_tensor should not None")
			return None

		vmode = vit_mode if vit_mode is not None else ov_vit_mode.VIT_MULTI_IMG_MODE
		img_tensor = check_image_type(img_tensor, vmode)
		if img_tensor is None:
			raise ValueError("check_image_type fail")

		user_ctx = self.multi_user_get(user_id)

		num_images = img_tensor.shape[0]
		self.infer.infer_user_preprocess(
			self.model_handle, user_ctx.handle, self.model_type, vmode, None, img_tensor, num_images)
		return None

	def tokenizer_text_image_token(
		self,
		prompt : str = None,
		img_tensor: torch.Tensor = None,
		vit_mode: Optional[int] = None,
		user_id: Optional[list] = None,
	):
		r"""This API will apply default system prompt for input text by Shepherd library,
		run vision tower model to get image embedding and then return the encoded token list which
		includes text tokens and image tokens.

		Args:
			prompt (`str`):
				Indices the input prompt text.
			img_tensor (`torch.Tensor`):
				Indices the input image data.
			vit_mode (`int`, *optional*):
				It's an extended configuration for Ambarella chips to index the vit mode for vision tower model.
				0: single image; 1: multi image; 2: video. Default is multi image mode.
			user_id (`list`, *optional*):
				It's an extended configuration for Ambarella chips to index the user ID for current inference.
				Users need specify this parameters if enable multi user.
		Returns (`torch.Tensor`):
			return the merged token list include text tokens and image tokens
		"""
		if prompt is None or img_tensor is None:
			logger.error("tokenizer_text_image_token: input with prompt and img_tensor should not None")
			return None

		vmode = vit_mode if vit_mode is not None else ov_vit_mode.VIT_MULTI_IMG_MODE
		img_tensor = check_image_type(img_tensor, vmode)
		if img_tensor is None:
			raise ValueError("check_image_type fail")

		user_ctx = self.multi_user_get(user_id)

		num_images = img_tensor.shape[0]
		input_ids = self.infer.infer_user_preprocess(
			self.model_handle, user_ctx.handle, self.model_type, vmode, prompt, img_tensor, num_images)
		return self.output_ids_cvt(input_ids)

	def generate(
		self,
		input_ids: Optional[torch.Tensor] = None,
		position: Optional[list] = None,
		user_id: Optional[list] = None,
		past_key_values: Optional[bool] = None,
		streamer: Optional[TextIteratorStreamer] = None,
		**kwargs: Dict,
	):
		r"""
		Args:
			input_ids (`torch.Tensor`):
				Indices the input data which inclde input text.
			position (`list`, *optional*):
				Indices to get of position of current conversation, like position=[], default is None.
			user_id (`list`, *optional*):
				It's an extended configuration for Ambarella chips to index the user ID for current inference.
				Users need specify this parameters if enable multi user.
			past_key_values (`bool`, *optional*):
				Indices the past conversation can be used for current inference.
			streamer (`TextIteratorStreamer`, *optional*):
				Streamer object that will be used to stream the generated sequences. Generated tokens are passed
				through `streamer.put(token_ids)` and the streamer is responsible for any further processing.
			kwargs (additional keyword arguments, *optional*):
				Compared to transformers, we currently only support some of the commonly used parameters as below:
					`do_sample`, `top_p`, `top_k`, and `temperature` are added to enable different sample methods
					with logits. `do_sample=None` will choose a hardware sample by default, `do_sample=False/True`
					will choose a different sample according to other configs like `top_p`, `top_k`.

					`max_length`, `max_new_tokens` are added to change the default max sequence length.
		Returns:
			`numpy.array`: the array of output token id

		Example for multi-image:

		.. code-block:: python

			import copy
			from PIL import Image

			from transformers import AutoTokenizer, AutoConfig
			from transformers_amba_ext import LlavaOnevisionForConditionalGeneration

			from transformers_amba_ext.models.llava_onevision.llava_next.mm_utils import process_images
			from transformers_amba_ext.models.llava_onevision.llava_next.siglip_encoder import SigLipImageProcessor

			model_path = "/home/lychee/cooper_max_demos/llm_demo/llava_onevision"
			model_config = AutoConfig.from_pretrained(model_path)
			model_config.image_aspect_ratio = "pad"
			tokenizer = AutoTokenizer.from_pretrained(model_path)
			model = LlavaOnevisionForConditionalGeneration.from_pretrained(model_path)

			image1 = Image.open("llava_v1_5_radar.jpg")
			image2 = Image.open("llava_logo.png")
			images = [image1, image2]
			image_processor = SigLipImageProcessor(image_mean=[0, 0, 0], image_std=[1, 1, 1], rescale_factor=1)
			image_tensors = process_images(images, image_processor, model_config)

			# Prepare interleaved text-image input
			question = f"<image> This is the first image. Can you describe what you see? Then, let's look at another image: <image> What's the difference between these two images?"

			# Below API will apply system prompt automatically by shepherd library
			input_ids = model.tokenizer_text_image_token(question, image_tensors)
			# Generate response
			cont = model.generate(
				input_ids,
				do_sample=None,
				temperature=0,
				max_new_tokens=512,
			)
			text_outputs = tokenizer.batch_decode(cont, skip_special_tokens=True)
			print(text_outputs[0])
		"""
		do_sample = kwargs.get("do_sample")

		input = self.input_ids_cvt(input_ids)
		if do_sample is not None:
			output = super().generate_logits(
				input_ids = input,
				position = position,
				user_id = user_id,
				past_key_values = past_key_values,
				streamer = streamer,
				kwargs = kwargs)
		else:
			output = super().generate_ids(
				input_ids = input,
				position = position,
				user_id = user_id,
				past_key_values = past_key_values,
				streamer = streamer,
				kwargs = kwargs)
		return self.output_ids_cvt(output)

	def to(self, device):
		r"""Bypass"""
		return self
