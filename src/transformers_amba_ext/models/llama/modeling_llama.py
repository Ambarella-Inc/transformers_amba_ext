
import numpy as np
import torch
from typing import Optional, Dict
from transformers import TextIteratorStreamer

from ..model_base.modeling_base import model_base
from ...utils import logging

logger = logging.get_logger(__name__)

class CausalLMOutput():
	logits = None
	past_key_values = None

class LlamaForCausalLM(model_base):
	def __init__(
		self,
		pretrained_model_path: Optional[str] = None,
		device_ip: Optional[str] = None,
		device_port: Optional[int] = None,
		log_level: Optional[int] = None,
		**kwargs,
	):
		super().__init__(
			pretrained_model_path = pretrained_model_path,
			device_ip = device_ip,
			device_port = device_port,
			log_level = log_level)

	@classmethod
	def from_pretrained(
		cls,
		pretrained_model_path: Optional[str] = None,
		device_ip: Optional[str] = None,
		device_port: Optional[int] = None,
		log_level: Optional[int] = None,
		**kwargs,
	):
		r"""
		Args:
			pretrained_model_path (`str`, *optional*):
				Indices the model path which generated by Ambarella tool.
			device_ip (`str`, *optional*):
				It's an extended configuration for Ambarella chips to index the IP address if enable RPC mode.
			device_port (`int`, *optional*):
				It's an extended configuration for Ambarella chips to index the port number if enable RPC mode.
			log_level (`int`, *optional*):
				It's an extended configuration for Ambarella chips to index the log level for Shepherd library.
		"""
		if kwargs:
			logger.warning(f"{cls.__name__}: unsupported kwargs: {kwargs}")
		return cls(pretrained_model_path, device_ip, device_port, log_level)

	def __del__(self):
		super().__del__()

	def generate(
		self,
		input_ids: torch.Tensor,
		position: Optional[list] = None,
		user_id: Optional[list] = None,
		past_key_values: Optional[bool] = None,
		streamer: Optional[TextIteratorStreamer] = None,
		**kwargs: Dict,
	):
		r"""
		Args:
			input_ids (`torch.LongTensor` of shape `(1, sequence_length)`):
				Indices of input sequence tokens in the vocabulary.
			position (`list`, *optional*):
				Indices to get of position of current conversation, like position=[], default is None.
			user_id (`list`, *optional*):
				It's an extended configuration for Ambarella chips to index the user ID for current inference.
				Users need specify this parameters if enable multi user.
			past_key_values (`bool`, *optional*):
				Indices the past conversation can be used for current inference.
			streamer (`TextIteratorStreamer`, *optional*):
				Streamer object that will be used to stream the generated sequences. Generated tokens are passed
				through `streamer.put(token_ids)` and the streamer is responsible for any further processing.
			kwargs (additional keyword arguments, *optional*):
				Compared to transformers, we currently only support some of the commonly used parameters as below:
					`do_sample`, `top_p`, `top_k`, and `temperature` are added to enable different sample methods
					with logits. `do_sample=None` will choose a hardware sample by default, `do_sample=False/True`
					will choose a different sample according to other configs like `top_p`, `top_k`.

					`max_length`, `max_new_tokens` are added to change the default max sequence length.
		Returns:
			`numpy.array`: the array of output token id.

		Example:

		.. code-block:: python

			from transformers import AutoTokenizer
			from transformers_amba_ext import LlamaForCausalLM

			model_path = "/home/lychee/cooper_max_demos/llm_demo/chatllama_7B/"
			model = LlamaForCausalLM.from_pretrained(model_path)
			tokenizer = AutoTokenizer.from_pretrained(model_path)

			prompt = "[INST] What is the height of Michael Jeffrey Jordan? [/INST]"
			inputs = tokenizer(prompt, return_tensors="pt")

			# Generate
			generate_ids = model.generate(inputs.input_ids)
			response = tokenizer.batch_decode(
				generate_ids[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
			print(response)
			" Michael Jordan's height is 6 feet 6 inches (198 cm) tall."

		Example for multi conversation:

		.. code-block:: python

			from transformers import AutoTokenizer
			from transformers_amba_ext import LlamaForCausalLM

			model_path = "/home/lychee/cooper_max_demos/llm_demo/chatllama_7B/"
			model = LlamaForCausalLM.from_pretrained(model_path)
			tokenizer = AutoTokenizer.from_pretrained(model_path)

			prompts = ["[INST] What is the height of Michael Jeffrey Jordan? [/INST]",
				"[INST] what is the height of Yao Ming? [/INST]"]

			pos = []
			for prompt in prompts:
				inputs = tokenizer(prompt, return_tensors="pt")
				generate_ids = model.generate(inputs.input_ids, position = pos, past_key_values = True)
				response = tokenizer.batch_decode(
					generate_ids[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
				print(f"[pos]: {pos[0]}\n{response}")
		"""

		logger.debug(f"generate kwargs: {kwargs}")
		do_sample = kwargs.get("do_sample")

		input = self.input_ids_cvt(input_ids)
		if do_sample is not None:
			output = super().generate_logits_until(
				input_ids = input,
				position = position,
				user_id = user_id,
				past_key_values = past_key_values,
				streamer = streamer,
				kwargs = kwargs)
		else:
			output = super().generate_ids_until(
				input_ids = input,
				position = position,
				user_id = user_id,
				past_key_values = past_key_values,
				streamer = streamer,
				kwargs = kwargs)
		return self.output_ids_cvt(np.concatenate((input, output)))

	def __call__(self,
		input_ids: torch.Tensor,
		position: Optional[list] = None,
		user_id: Optional[list] = None,
		past_key_values: Optional[bool] = None,
		streamer: Optional[TextIteratorStreamer] = None,
		**kwargs: Dict,
	):
		input = self.input_ids_cvt(input_ids)
		inplen = input.shape[-1]
		use_past_key_values = past_key_values if past_key_values is not None else False

		logits, pos = super().generate_logits(
			input_ids = input, user_id = user_id)

		if use_past_key_values == False:
			pos = self.reset(user_id)

		logits = np.repeat(logits, inplen, axis=0)
		logits = torch.tensor(logits.reshape(1, inplen, -1), dtype=torch.float32)

		causal_out = CausalLMOutput()
		causal_out.logits = logits
		return causal_out

	def to(self, device):
		r"""Bypass"""
		return self
